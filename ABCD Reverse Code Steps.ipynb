{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39de496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reverse Geocode Steps\n",
    "\n",
    "# Step One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7731f716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbfd56de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step One ## Using SVI variables to reverse code census tract id\n",
    "# read abcd svi data\n",
    "abcd_svi = pd.read_csv('./data/led_l_svi.csv')\n",
    "\n",
    "# read CDC SVI data\n",
    "CDC_svi = pd.read_csv('./data/SVI_2018_US.csv')\n",
    "# https://www.atsdr.cdc.gov/placeandhealth/svi/documentation/SVI_documentation_2018.html\n",
    "\n",
    "# select the variables to match\n",
    "# Columns to match for obtaining unique census tract\n",
    "abcd_columns_id = [\n",
    "    'reshist_addr1_svi_pov_20142018',\n",
    "    'reshist_addr1_svi_emp_20142018',\n",
    "    'reshist_addr1_svi_cap_20142018',\n",
    "    'reshist_addr1_svi_hs_20142018',\n",
    "    'reshist_addr1_svi_65_20142018',\n",
    "    'reshist_addr1_svi_17_20142018',\n",
    "    'reshist_addr1_svi_dis_20142018',\n",
    "    'reshist_addr1_svi_sin_20142018',\n",
    "    'reshist_addr1_svi_min_20142018',\n",
    "    'reshist_addr1_svi_eng_20142018',\n",
    "    'reshist_addr1_svi_hous20142018',\n",
    "    'reshist_addr1_svi_mob_20142018',\n",
    "    'reshist_addr1_svi_crwd20142018',\n",
    "    'reshist_addr1_svi_veh_20142018',\n",
    "    'reshist_addr1_svi_grp_20142018',\n",
    "    'src_subject_id'  \n",
    "]\n",
    "\n",
    "abcd_columns = [\n",
    "    'reshist_addr1_svi_pov_20142018',\n",
    "    'reshist_addr1_svi_emp_20142018',\n",
    "    'reshist_addr1_svi_cap_20142018',\n",
    "    'reshist_addr1_svi_hs_20142018',\n",
    "    'reshist_addr1_svi_65_20142018',\n",
    "    'reshist_addr1_svi_17_20142018',\n",
    "    'reshist_addr1_svi_dis_20142018',\n",
    "    'reshist_addr1_svi_sin_20142018',\n",
    "    'reshist_addr1_svi_min_20142018',\n",
    "    'reshist_addr1_svi_eng_20142018',\n",
    "    'reshist_addr1_svi_hous20142018',\n",
    "    'reshist_addr1_svi_mob_20142018',\n",
    "    'reshist_addr1_svi_crwd20142018',\n",
    "    'reshist_addr1_svi_veh_20142018',\n",
    "    'reshist_addr1_svi_grp_20142018'\n",
    "]\n",
    "\n",
    "\n",
    "cdc_columns = [\n",
    "    'EPL_POV',\n",
    "    'EPL_UNEMP',\n",
    "    'EPL_PCI',\n",
    "    'EPL_NOHSDP',\n",
    "    'EPL_AGE65',\n",
    "    'EPL_AGE17',\n",
    "    'EPL_DISABL',\n",
    "    'EPL_SNGPNT',\n",
    "    'EPL_MINRTY',\n",
    "    'EPL_LIMENG',\n",
    "    'EPL_MUNIT',\n",
    "    'EPL_MOBILE',\n",
    "    'EPL_CROWD',\n",
    "    'EPL_NOVEH',\n",
    "    'EPL_GROUPQ',\n",
    "    'STATE','COUNTY','FIPS','LOCATION','AREA_SQMI'\n",
    "]\n",
    "\n",
    "# ABCD selected with id\n",
    "abcd_svi_selected_id = abcd_svi[abcd_columns_id]\n",
    "\n",
    "# Select relevant columns from ABCD SVI dataset\n",
    "abcd_svi_selected = abcd_svi[abcd_columns]\n",
    "\n",
    "# Select relevant columns from CDC SVI dataset\n",
    "cdc_svi_selected = CDC_svi[cdc_columns]\n",
    "\n",
    "# Rename columns in ABCD SVI to match CDC SVI columns\n",
    "abcd_svi_selected.columns = cdc_columns[:-5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb5d142c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique census tracts based on matching columns: 72405\n",
      "Total number of records in CDC SVI dataset: 72837\n",
      "There are non-unique combinations of selected columns.\n",
      "Non-unique combinations of selected columns:\n",
      "       EPL_POV  EPL_UNEMP  EPL_PCI  EPL_NOHSDP  EPL_AGE65  EPL_AGE17  \\\n",
      "0       -999.0     -999.0   -999.0      -999.0     -999.0  -999.0000   \n",
      "1       -999.0     -999.0   -999.0      -999.0     -999.0  -999.0000   \n",
      "2       -999.0     -999.0   -999.0      -999.0     -999.0  -999.0000   \n",
      "4       -999.0     -999.0   -999.0      -999.0     -999.0  -999.0000   \n",
      "5       -999.0     -999.0   -999.0      -999.0     -999.0  -999.0000   \n",
      "...        ...        ...      ...         ...        ...        ...   \n",
      "34592      0.0        0.0   -999.0         0.0        0.0     0.0000   \n",
      "37481      0.0        0.0   -999.0         0.0        0.0     0.0000   \n",
      "37651      0.0        0.0   -999.0         0.0        0.0     0.9993   \n",
      "38691      0.0        0.0   -999.0         0.0        0.0     0.0000   \n",
      "42005      0.0        0.0   -999.0         0.0        0.0     0.0000   \n",
      "\n",
      "       EPL_DISABL  EPL_SNGPNT  EPL_MINRTY  EPL_LIMENG  EPL_MUNIT  EPL_MOBILE  \\\n",
      "0          -999.0      -999.0   -999.0000      -999.0     -999.0      -999.0   \n",
      "1          -999.0      -999.0   -999.0000      -999.0     -999.0      -999.0   \n",
      "2          -999.0      -999.0   -999.0000      -999.0     -999.0      -999.0   \n",
      "4          -999.0      -999.0   -999.0000      -999.0     -999.0      -999.0   \n",
      "5          -999.0      -999.0   -999.0000      -999.0     -999.0      -999.0   \n",
      "...           ...         ...         ...         ...        ...         ...   \n",
      "34592         0.0         0.0      0.0000         0.0        0.0         0.0   \n",
      "37481         0.0         0.0      0.0000         0.0        0.0         0.0   \n",
      "37651         0.0         0.0      0.0000         0.0        0.0         0.0   \n",
      "38691         0.0         0.0      0.6816         0.0        0.0         0.0   \n",
      "42005         0.0         0.0      0.0000         0.0        0.0         0.0   \n",
      "\n",
      "       EPL_CROWD  EPL_NOVEH  EPL_GROUPQ         FIPS  \n",
      "0         -999.0     -999.0      -999.0   1015981901  \n",
      "1         -999.0     -999.0      -999.0   1015981902  \n",
      "2         -999.0     -999.0      -999.0   1015981903  \n",
      "4         -999.0     -999.0      -999.0   1097990000  \n",
      "5         -999.0     -999.0      -999.0   1117980000  \n",
      "...          ...        ...         ...          ...  \n",
      "34592        0.0        0.0         0.0  42003980700  \n",
      "37481        0.0        0.0         0.0  47031980100  \n",
      "37651        0.0        0.0         0.0  47053980100  \n",
      "38691        0.0        0.0         0.0  48029980005  \n",
      "42005        0.0        0.0         0.0  51013980100  \n",
      "\n",
      "[436 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check for uniqueness\n",
    "matching_columns = [\n",
    "    'EPL_POV',\n",
    "    'EPL_UNEMP',\n",
    "    'EPL_PCI',\n",
    "    'EPL_NOHSDP',\n",
    "    'EPL_AGE65',\n",
    "    'EPL_AGE17',\n",
    "    'EPL_DISABL',\n",
    "    'EPL_SNGPNT',\n",
    "    'EPL_MINRTY',\n",
    "    'EPL_LIMENG',\n",
    "    'EPL_MUNIT',\n",
    "    'EPL_MOBILE',\n",
    "    'EPL_CROWD',\n",
    "    'EPL_NOVEH',\n",
    "    'EPL_GROUPQ'\n",
    "]\n",
    "unique_tracts = CDC_svi.drop_duplicates(subset=matching_columns)\n",
    "print(\"Number of unique census tracts based on matching columns:\", len(unique_tracts))\n",
    "print(\"Total number of records in CDC SVI dataset:\", len(CDC_svi))\n",
    "\n",
    "if len(unique_tracts) == len(CDC_svi):\n",
    "    print(\"Each combination of selected columns corresponds to a unique FIPS ID.\") # 72405\n",
    "else:\n",
    "    print(\"There are non-unique combinations of selected columns.\") # 72837 \n",
    "    \n",
    "    # Identify non-unique combinations\n",
    "duplicates = CDC_svi[CDC_svi.duplicated(subset=matching_columns, keep=False)]\n",
    "\n",
    "# Print the non-unique combinations\n",
    "print(\"Non-unique combinations of selected columns:\")\n",
    "print(duplicates[matching_columns + ['FIPS']])\n",
    "\n",
    "# Save the duplicates to a CSV file\n",
    "duplicates.to_csv('./data/cdc_svi_duplicates.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca885d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No overlapping FIPS codes found.\n"
     ]
    }
   ],
   "source": [
    "# Merge the datasets based on the selected columns to get FIPS (GEOID)\n",
    "merged_df = pd.merge(abcd_svi_selected_id, cdc_svi_selected, left_on=abcd_columns, right_on=cdc_columns[:-5], how='left')\n",
    "\n",
    "# Reorder columns to place 'src_subject_id' first\n",
    "final_columns = ['src_subject_id'] + [col for col in merged_df.columns if col != 'src_subject_id']\n",
    "merged_df = merged_df[final_columns]\n",
    "\n",
    "## check if any of the FIPS were the same as FIPS from the duplicates \n",
    "# Check for overlapping FIPS codes between unique and duplicate entries\n",
    "overlap_fips = duplicates['FIPS'].isin(merged_df['FIPS']).any()\n",
    "\n",
    "if overlap_fips:\n",
    "    print(\"There are overlapping FIPS codes between the duplicate and merged datasets.\")\n",
    "else:\n",
    "    print(\"No overlapping FIPS codes found.\")\n",
    "\n",
    "# Optionally, to view the specific overlapping FIPS codes\n",
    "if overlap_fips:\n",
    "    overlapping_fips = duplicates[duplicates['FIPS'].isin(merged_df['FIPS'])]['FIPS']\n",
    "    print(\"Overlapping FIPS codes:\", overlapping_fips.unique())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "106ea806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No replicates, then save the merged file \n",
    "merged_df.to_csv('./data/merged_abcd_svi_with_fips.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dda9d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step Two "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e597cbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using walkability index and gross residential density to locate census block group within the census tract \n",
    "## \n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import fiona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78dde24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        GEOID10  TRFIPS CFIPS SFIPS   CSA CSA_Name     CBSA       CBSA_Name  \\\n",
      "0  010059505002  950500   005    01  None     None  21640.0  Eufaula, AL-GA   \n",
      "1  010059505001  950500   005    01  None     None  21640.0  Eufaula, AL-GA   \n",
      "2  010059502001  950200   005    01  None     None  21640.0  Eufaula, AL-GA   \n",
      "3  010059502002  950200   005    01  None     None  21640.0  Eufaula, AL-GA   \n",
      "4  010059504002  950400   005    01  None     None  21640.0  Eufaula, AL-GA   \n",
      "\n",
      "   CBSA_EMP  CBSA_POP  ...  D5cri  D5ce  D5cei     D5dr    D5dri     D5de  \\\n",
      "0    9921.0   29970.0  ...   0.66  4.24   0.68 -99999.0 -99999.0 -99999.0   \n",
      "1    9921.0   29970.0  ...   0.67  4.70   0.75 -99999.0 -99999.0 -99999.0   \n",
      "2    9921.0   29970.0  ...   0.30  2.60   0.42 -99999.0 -99999.0 -99999.0   \n",
      "3    9921.0   29970.0  ...   0.27  2.58   0.41 -99999.0 -99999.0 -99999.0   \n",
      "4    9921.0   29970.0  ...   0.32  3.35   0.54 -99999.0 -99999.0 -99999.0   \n",
      "\n",
      "     D5dei  Shape_Length  Shape_Area  \\\n",
      "0 -99999.0      0.846952    0.018273   \n",
      "1 -99999.0      0.521148    0.005095   \n",
      "2 -99999.0      0.660835    0.018740   \n",
      "3 -99999.0      0.695057    0.020340   \n",
      "4 -99999.0      0.499117    0.008829   \n",
      "\n",
      "                                            geometry  \n",
      "0  MULTIPOLYGON (((-85.27760 31.85656, -85.27743 ...  \n",
      "1  MULTIPOLYGON (((-85.15785 31.82915, -85.15698 ...  \n",
      "2  MULTIPOLYGON (((-85.39410 32.02565, -85.39335 ...  \n",
      "3  MULTIPOLYGON (((-85.46273 32.01732, -85.46254 ...  \n",
      "4  MULTIPOLYGON (((-85.58513 31.72677, -85.58563 ...  \n",
      "\n",
      "[5 rows x 117 columns]\n"
     ]
    }
   ],
   "source": [
    "# get the gross population density \n",
    "gdb = './SLD/SMARTLOCATIONDB/SmartLocationDb.gdb'\n",
    "fc = gpd.read_file(gdb)\n",
    "# Specify the correct layer name\n",
    "layer_name = 'TIGER2010_bg_SLD'\n",
    "# Read the specified layer\n",
    "fc = gpd.read_file(gdb, layer=layer_name)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(fc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b5ca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally \n",
    "\n",
    "## check the gross residential density for Geoid \n",
    "# geoid of 601441800 look at all dataset that start with this geoid \n",
    "# Filter the DataFrame for GEOID that starts with '601441800'\n",
    "#filtered_df = fc[fc['GEOID10'].str.startswith('06001441800')]\n",
    "\n",
    "# Filter the DataFrame for GEOID10 that starts with '06'\n",
    "filtered_df = fc[fc['GEOID10'].str.startswith('06001441800')]\n",
    "\n",
    "# Display the first few rows to inspect similar patterns\n",
    "print(filtered_df[['GEOID10']].head(10))\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "# Set Pandas to display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Display the first few rows of the filtered DataFrame to inspect all columns\n",
    "print(filtered_df.head(10))\n",
    "\n",
    "## Matched !!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83562d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows with Duplicate D1A values within the same 10-digit GEOID groups:\n",
      "             GEOID10  TRFIPS CFIPS SFIPS   CSA CSA_Name     CBSA   CBSA_Name  \\\n",
      "6977    040190005001  000500   019    04  None     None  46060.0  Tucson, AZ   \n",
      "7073    040190005005  000500   019    04  None     None  46060.0  Tucson, AZ   \n",
      "7276    040279800031  980003   027    04  None     None  49740.0    Yuma, AZ   \n",
      "7365    040279800041  980004   027    04  None     None  49740.0    Yuma, AZ   \n",
      "7382    040279800051  980005   027    04  None     None  49740.0    Yuma, AZ   \n",
      "...              ...     ...   ...   ...   ...      ...      ...         ...   \n",
      "220648  691209501021  950102   120    69  None     None      0.0        None   \n",
      "220649  691209501011  950101   120    69  None     None      0.0        None   \n",
      "220650  691209502003  950200   120    69  None     None      0.0        None   \n",
      "220651  691209502002  950200   120    69  None     None      0.0        None   \n",
      "220652  691209502001  950200   120    69  None     None      0.0        None   \n",
      "\n",
      "        CBSA_EMP  CBSA_POP  ...      D5ce  D5cei     D5dr    D5dri     D5de  \\\n",
      "6977    345317.0  980263.0  ...      0.22   0.90 -99999.0 -99999.0 -99999.0   \n",
      "7073    345317.0  980263.0  ...      0.20   0.84 -99999.0 -99999.0 -99999.0   \n",
      "7276     61284.0  195751.0  ...      0.00   0.00 -99999.0 -99999.0 -99999.0   \n",
      "7365     61284.0  195751.0  ...      0.00   0.00 -99999.0 -99999.0 -99999.0   \n",
      "7382     61284.0  195751.0  ...      0.29   0.27 -99999.0 -99999.0 -99999.0   \n",
      "...          ...       ...  ...       ...    ...      ...      ...      ...   \n",
      "220648       0.0       0.0  ... -99999.00   0.00 -99999.0 -99999.0 -99999.0   \n",
      "220649       0.0       0.0  ... -99999.00   0.00 -99999.0 -99999.0 -99999.0   \n",
      "220650       0.0       0.0  ... -99999.00   0.00 -99999.0 -99999.0 -99999.0   \n",
      "220651       0.0       0.0  ... -99999.00   0.00 -99999.0 -99999.0 -99999.0   \n",
      "220652       0.0       0.0  ... -99999.00   0.00 -99999.0 -99999.0 -99999.0   \n",
      "\n",
      "          D5dei  Shape_Length  Shape_Area  \\\n",
      "6977   -99999.0      0.012587    0.000006   \n",
      "7073   -99999.0      0.014901    0.000009   \n",
      "7276   -99999.0      1.860400    0.177578   \n",
      "7365   -99999.0      3.360370    0.376096   \n",
      "7382   -99999.0      0.396585    0.005418   \n",
      "...         ...           ...         ...   \n",
      "220648 -99999.0      0.133058    0.000589   \n",
      "220649 -99999.0      0.045620    0.000090   \n",
      "220650 -99999.0      0.006007    0.000002   \n",
      "220651 -99999.0      0.448782    0.005332   \n",
      "220652 -99999.0      0.341580    0.002767   \n",
      "\n",
      "                                                 geometry  GEOID10_10  \n",
      "6977    MULTIPOLYGON (((-110.95231 32.23378, -110.9523...  0401900050  \n",
      "7073    MULTIPOLYGON (((-110.94915 32.23381, -110.9483...  0401900050  \n",
      "7276    MULTIPOLYGON (((-113.91668 32.42150, -113.9062...  0402798000  \n",
      "7365    MULTIPOLYGON (((-113.33372 32.76786, -113.3337...  0402798000  \n",
      "7382    MULTIPOLYGON (((-114.64814 32.49473, -114.6474...  0402798000  \n",
      "...                                                   ...         ...  \n",
      "220648  MULTIPOLYGON (((145.53368 14.83494, 145.53367 ...  6912095010  \n",
      "220649  MULTIPOLYGON (((145.63147 14.97077, 145.63156 ...  6912095010  \n",
      "220650  MULTIPOLYGON (((145.62432 14.95137, 145.62418 ...  6912095020  \n",
      "220651  MULTIPOLYGON (((145.64514 15.10116, 145.64532 ...  6912095020  \n",
      "220652  MULTIPOLYGON (((145.63752 15.03489, 145.63764 ...  6912095020  \n",
      "\n",
      "[508 rows x 118 columns]\n",
      "\n",
      "Number of rows with D1A value of 0 within the duplicates:\n",
      "508\n"
     ]
    }
   ],
   "source": [
    "# Check how many of the census block groups have the same d1a within the census tract (first 10 digits identical geoid)\n",
    "fc['GEOID10_10'] = fc['GEOID10'].str[:10]\n",
    "\n",
    "# Step 2: Group by the first 10 digits of GEOID10 and check for duplicate D1A values\n",
    "duplicate_d1a_within_groups = fc[fc.duplicated(subset=['GEOID10_10', 'D1A'], keep=False)]\n",
    "print(\"\\nRows with Duplicate D1A values within the same 10-digit GEOID groups:\")\n",
    "print(duplicate_d1a_within_groups)\n",
    "\n",
    "len(duplicate_d1a_within_groups) # 508 duplicates\n",
    "\n",
    "duplicates_with_zero_d1a = duplicate_d1a_within_groups[duplicate_d1a_within_groups['D1A'] == 0]\n",
    "\n",
    "count_zero_d1a = duplicates_with_zero_d1a.shape[0]\n",
    "print(\"\\nNumber of rows with D1A value of 0 within the duplicates:\")\n",
    "print(count_zero_d1a) # 508 were all 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "071ff540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          BlkGrpID\n",
      "11133  60014418001\n",
      "11134  60014418002\n",
      "11135  60014418003\n",
      "11136  60014418004\n",
      "          BlkGrpID  StCoFIPS StAbbr  NatWalkInd  Pop2010  HU2010  HH2010  \\\n",
      "11133  60014418001      6001     CA      16.167     2595     911     867   \n",
      "11134  60014418002      6001     CA      16.167     1235     421     410   \n",
      "11135  60014418003      6001     CA      14.667     2069     799     756   \n",
      "11136  60014418004      6001     CA      12.667      887     282     277   \n",
      "\n",
      "       D2A_EPHHM  D2B_E8MIXA      D3B         D4A  D2A_Ranked  D2B_Ranked  \\\n",
      "11133   0.496664    0.847142  134.269  820.482232          11          20   \n",
      "11134   0.485309    0.571528  138.259  238.591937          11          12   \n",
      "11135   0.464759    0.587045  108.353  649.157072          10          12   \n",
      "11136   0.336812    0.000000  124.141  711.533729           7           1   \n",
      "\n",
      "       D3B_Ranked  D4A_Ranked  \n",
      "11133          18          15  \n",
      "11134          18          19  \n",
      "11135          17          16  \n",
      "11136          18          16  \n",
      "      BlkGrpID  StCoFIPS StAbbr  NatWalkInd  Pop2010  HU2010  HH2010  \\\n",
      "0  10010201001      1001     AL       7.000      698     283     262   \n",
      "1  10010201002      1001     AL       7.833     1214     469     431   \n",
      "2  10010202001      1001     AL       7.500     1003     375     341   \n",
      "3  10010202002      1001     AL      10.167     1167     447     402   \n",
      "4  10010203001      1001     AL       3.167     2549     963     915   \n",
      "\n",
      "   D2A_EPHHM  D2B_E8MIXA      D3B      D4A  D2A_Ranked  D2B_Ranked  \\\n",
      "0   0.729616    0.544996  11.4319 -99999.0          17          11   \n",
      "1   0.573856    0.913864  13.0227 -99999.0          13          20   \n",
      "2   0.628905    0.499410  37.3242 -99999.0          15          10   \n",
      "3   0.741931    0.729355  59.2237 -99999.0          18          17   \n",
      "4   0.125196    0.000000  24.0513 -99999.0           2           1   \n",
      "\n",
      "   D3B_Ranked  D4A_Ranked  \n",
      "0           6           1  \n",
      "1           6           1  \n",
      "2           9           1  \n",
      "3          12           1  \n",
      "4           7           1  \n"
     ]
    }
   ],
   "source": [
    "## import walkability\n",
    "# Make sure to use excel sheet and make sure the geoids were in 12 digits format\n",
    "\n",
    "walk = pd.read_excel('./data/WalkIndex_USBlkGrps.xlsx')\n",
    "\n",
    "walk['BlkGrpID'] = walk['BlkGrpID'].astype(str)\n",
    "# Filter the DataFrame for GEOID10 that starts with '06'\n",
    "filtered_walk = walk[walk['BlkGrpID'].str.startswith('6001441800')]\n",
    "\n",
    "# Display the first few rows to inspect similar patterns\n",
    "print(filtered_walk[['BlkGrpID']].head(10))\n",
    "\n",
    "print(filtered_walk.head(10)) # matched\n",
    "\n",
    "len(walk) #220334\n",
    "\n",
    "print(walk.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d61bede6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       BlkGrpID  StCoFIPS StAbbr  NatWalkInd  Pop2010  HU2010  HH2010  \\\n",
      "0  010010201001      1001     AL       7.000      698     283     262   \n",
      "1  010010201002      1001     AL       7.833     1214     469     431   \n",
      "2  010010202001      1001     AL       7.500     1003     375     341   \n",
      "3  010010202002      1001     AL      10.167     1167     447     402   \n",
      "4  010010203001      1001     AL       3.167     2549     963     915   \n",
      "\n",
      "   D2A_EPHHM  D2B_E8MIXA      D3B      D4A  D2A_Ranked  D2B_Ranked  \\\n",
      "0   0.729616    0.544996  11.4319 -99999.0          17          11   \n",
      "1   0.573856    0.913864  13.0227 -99999.0          13          20   \n",
      "2   0.628905    0.499410  37.3242 -99999.0          15          10   \n",
      "3   0.741931    0.729355  59.2237 -99999.0          18          17   \n",
      "4   0.125196    0.000000  24.0513 -99999.0           2           1   \n",
      "\n",
      "   D3B_Ranked  D4A_Ranked  \n",
      "0           6           1  \n",
      "1           6           1  \n",
      "2           9           1  \n",
      "3          12           1  \n",
      "4           7           1  \n",
      "        GEOID10  TRFIPS CFIPS SFIPS   CSA CSA_Name     CBSA       CBSA_Name  \\\n",
      "0  010059505002  950500   005    01  None     None  21640.0  Eufaula, AL-GA   \n",
      "1  010059505001  950500   005    01  None     None  21640.0  Eufaula, AL-GA   \n",
      "2  010059502001  950200   005    01  None     None  21640.0  Eufaula, AL-GA   \n",
      "3  010059502002  950200   005    01  None     None  21640.0  Eufaula, AL-GA   \n",
      "4  010059504002  950400   005    01  None     None  21640.0  Eufaula, AL-GA   \n",
      "\n",
      "   CBSA_EMP  CBSA_POP  ...  D5ce  D5cei     D5dr    D5dri     D5de    D5dei  \\\n",
      "0    9921.0   29970.0  ...  4.24   0.68 -99999.0 -99999.0 -99999.0 -99999.0   \n",
      "1    9921.0   29970.0  ...  4.70   0.75 -99999.0 -99999.0 -99999.0 -99999.0   \n",
      "2    9921.0   29970.0  ...  2.60   0.42 -99999.0 -99999.0 -99999.0 -99999.0   \n",
      "3    9921.0   29970.0  ...  2.58   0.41 -99999.0 -99999.0 -99999.0 -99999.0   \n",
      "4    9921.0   29970.0  ...  3.35   0.54 -99999.0 -99999.0 -99999.0 -99999.0   \n",
      "\n",
      "   Shape_Length  Shape_Area  \\\n",
      "0      0.846952    0.018273   \n",
      "1      0.521148    0.005095   \n",
      "2      0.660835    0.018740   \n",
      "3      0.695057    0.020340   \n",
      "4      0.499117    0.008829   \n",
      "\n",
      "                                            geometry  GEOID10_10  \n",
      "0  MULTIPOLYGON (((-85.27760 31.85656, -85.27743 ...  0100595050  \n",
      "1  MULTIPOLYGON (((-85.15785 31.82915, -85.15698 ...  0100595050  \n",
      "2  MULTIPOLYGON (((-85.39410 32.02565, -85.39335 ...  0100595020  \n",
      "3  MULTIPOLYGON (((-85.46273 32.01732, -85.46254 ...  0100595020  \n",
      "4  MULTIPOLYGON (((-85.58513 31.72677, -85.58563 ...  0100595040  \n",
      "\n",
      "[5 rows x 118 columns]\n",
      "Maximum ID for walk: 721537506022\n",
      "Maximum ID for fc: 721537506022\n"
     ]
    }
   ],
   "source": [
    "fc['GEOID10'] = fc['GEOID10'].astype(str)\n",
    "walk['BlkGrpID'] = walk['BlkGrpID'].astype(str)\n",
    "\n",
    "\n",
    "# Remove decimal points from 'BlkGrpID' and convert to string\n",
    "walk['BlkGrpID'] = walk['BlkGrpID'].apply(lambda x: str(int(float(x))))\n",
    "walk['BlkGrpID'] = walk['BlkGrpID'].str.zfill(12)\n",
    "print(walk.head())\n",
    "\n",
    "\n",
    "#fc['GEOID10'] = fc['GEOID10'].str.zfill(12)\n",
    "print(fc.head())\n",
    "\n",
    "# For 'walk', assuming 'BlkGrpID' is the ID column\n",
    "max_walk_id = walk['BlkGrpID'].max()\n",
    "\n",
    "# For 'fc', assuming 'GEOID10' is the ID column\n",
    "max_fc_id = fc['GEOID10'].max()\n",
    "\n",
    "# Print the maximum IDs\n",
    "print(f\"Maximum ID for walk: {max_walk_id}\")\n",
    "print(f\"Maximum ID for fc: {max_fc_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbc1cfbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220334"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Combine Walkability and D1a based on BlkGrpID (Walk) and ID (fc)\n",
    "len(fc) #220653\n",
    "len(walk) #220334\n",
    "\n",
    "# make sure to retain most of the IDs, so left join onto fc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cff0dbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching IDs: 220263\n",
      "Number of non-matching IDs: 390\n",
      "Non-matching Records:\n",
      "             GEOID10 BlkGrpID       D1A  NatWalkInd\n",
      "3447    022700001001      NaN  0.000235         NaN\n",
      "3448    022700001004      NaN  0.000225         NaN\n",
      "3449    022700001003      NaN  0.000498         NaN\n",
      "3450    022700001002      NaN  0.000081         NaN\n",
      "6404    040194704002      NaN  0.013269         NaN\n",
      "...              ...      ...       ...         ...\n",
      "220648  691209501021      NaN  0.000000         NaN\n",
      "220649  691209501011      NaN  0.000000         NaN\n",
      "220650  691209502003      NaN  0.000000         NaN\n",
      "220651  691209502002      NaN  0.000000         NaN\n",
      "220652  691209502001      NaN  0.000000         NaN\n",
      "\n",
      "[390 rows x 4 columns]\n",
      "Unique state codes from non-matching records:\n",
      "['02' '04' '06' '36' '46' '51' '60' '66' '69']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qyuan25\\AppData\\Local\\Temp\\ipykernel_44748\\2630858197.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_matching_records['StateCode'] = non_matching_records['GEOID10'].str[:2]\n"
     ]
    }
   ],
   "source": [
    "# Select only BlkGrpID and NatWalkInd from walk\n",
    "walk_reduced = walk[['BlkGrpID', 'NatWalkInd']]\n",
    "\n",
    "# Select only GEOID10 and D1A from fc\n",
    "fc_reduced = fc[['GEOID10', 'D1A']]\n",
    "\n",
    "# delete states that start with 02, any state >= 55\n",
    "# Filter out states starting with '02' and any state with codes >= 55\n",
    "#fc_reduced = fc_reduced[~(fc_reduced['GEOID10'].str.startswith('02') | (fc_reduced['GEOID10'].str[:2].astype(int) >= 55))]\n",
    "\n",
    "# How many of them matched \n",
    "# Perform the left join\n",
    "combined_data = fc_reduced.merge(walk_reduced, how='left', left_on='GEOID10', right_on='BlkGrpID')\n",
    "\n",
    "# export the walk_reduced in csv\n",
    "#walk_reduced.to_csv('walk_reduced.csv', index=False)  # Set index=False to not include row indices in the file\n",
    "\n",
    "# Add a column to check if the IDs match\n",
    "combined_data['ID_Match'] = combined_data['BlkGrpID'] == combined_data['GEOID10']\n",
    "\n",
    "# Count matching and non-matching records\n",
    "matching_count = combined_data['ID_Match'].sum()\n",
    "non_matching_count = (~combined_data['ID_Match']).sum()\n",
    "\n",
    "print(f\"Number of matching IDs: {matching_count}\")\n",
    "print(f\"Number of non-matching IDs: {non_matching_count}\")\n",
    "\n",
    "# Display non-matching records\n",
    "non_matching_records = combined_data[~combined_data['ID_Match']]\n",
    "\n",
    "print(\"Non-matching Records:\")\n",
    "print(non_matching_records[['GEOID10', 'BlkGrpID', 'D1A', 'NatWalkInd']])\n",
    "\n",
    "# print out the first two digits of the nonmatching records\n",
    "# Extract the first two digits of GEOID10 for non-matching records\n",
    "non_matching_records['StateCode'] = non_matching_records['GEOID10'].str[:2]\n",
    "\n",
    "# Extract unique state codes from non-matching records\n",
    "unique_state_codes = non_matching_records['StateCode'].unique()\n",
    "\n",
    "# Print the unique state codes\n",
    "print(\"Unique state codes from non-matching records:\")\n",
    "print(unique_state_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3f5494f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Data:\n",
      "        GEOID10       D1A      BlkGrpID  NatWalkInd\n",
      "0  010059505002  0.011064  010059505002       3.000\n",
      "1  010059505001  0.084972  010059505001       4.500\n",
      "2  010059502001  0.008673  010059502001       5.667\n",
      "3  010059502002  0.006218  010059502002       3.833\n",
      "4  010059504002  0.021627  010059504002       5.667\n",
      "\n",
      "Length of fc: 220653\n",
      "Length of combined data: 220653\n"
     ]
    }
   ],
   "source": [
    "# Perform the left join\n",
    "combined_data = fc_reduced.merge(walk_reduced, how='left', left_on='GEOID10', right_on='BlkGrpID')\n",
    "# Display the result\n",
    "print(\"Combined Data:\")\n",
    "print(combined_data.head())\n",
    "\n",
    "# Check the lengths to ensure all records from fc are retained\n",
    "print(f\"\\nLength of fc: {len(fc)}\")\n",
    "print(f\"Length of combined data: {len(combined_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f133fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of successful matches: 220263\n",
      "Number of unsuccessful matches (missing walkability data): 390\n"
     ]
    }
   ],
   "source": [
    "## Check how many of them has been connected\n",
    "# Check successful and unsuccessful matches\n",
    "successful_matches = combined_data['NatWalkInd'].notna().sum()\n",
    "unsuccessful_matches = combined_data['NatWalkInd'].isna().sum()\n",
    "\n",
    "# Display the results\n",
    "print(f\"Number of successful matches: {successful_matches}\")\n",
    "print(f\"Number of unsuccessful matches (missing walkability data): {unsuccessful_matches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88198cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names and count for abcd_walk:\n",
      "Index(['src_subject_id', 'eventname', 'reshist_addr1_walkindex',\n",
      "       'reshist_addr2_walkindex', 'reshist_addr3_walkindex'],\n",
      "      dtype='object')\n",
      "Total columns: 11216\n",
      "\n",
      "Column names and count for abcd_d1a:\n",
      "Index(['src_subject_id', 'eventname', 'reshist_addr1_d1a', 'reshist_addr2_d1a',\n",
      "       'reshist_addr3_d1a'],\n",
      "      dtype='object')\n",
      "Total columns: 11216\n",
      "\n",
      "Column names and count for abcd_fips:\n",
      "Index(['src_subject_id', 'STATE', 'COUNTY', 'FIPS', 'LOCATION', 'AREA_SQMI',\n",
      "       'reshist_addr1_svi_pov_20142018', 'reshist_addr1_svi_emp_20142018',\n",
      "       'reshist_addr1_svi_cap_20142018', 'reshist_addr1_svi_hs_20142018',\n",
      "       'reshist_addr1_svi_65_20142018', 'reshist_addr1_svi_17_20142018',\n",
      "       'reshist_addr1_svi_dis_20142018', 'reshist_addr1_svi_sin_20142018',\n",
      "       'reshist_addr1_svi_min_20142018', 'reshist_addr1_svi_eng_20142018',\n",
      "       'reshist_addr1_svi_hous20142018', 'reshist_addr1_svi_mob_20142018',\n",
      "       'reshist_addr1_svi_crwd20142018', 'reshist_addr1_svi_veh_20142018',\n",
      "       'reshist_addr1_svi_grp_20142018', 'EPL_POV', 'EPL_UNEMP', 'EPL_PCI',\n",
      "       'EPL_NOHSDP', 'EPL_AGE65', 'EPL_AGE17', 'EPL_DISABL', 'EPL_SNGPNT',\n",
      "       'EPL_MINRTY', 'EPL_LIMENG', 'EPL_MUNIT', 'EPL_MOBILE', 'EPL_CROWD',\n",
      "       'EPL_NOVEH', 'EPL_GROUPQ'],\n",
      "      dtype='object')\n",
      "Total columns: 11232\n"
     ]
    }
   ],
   "source": [
    "## import ABCD walkability and gross residential density index \n",
    "\n",
    "abcd_walk = pd.read_csv('./data/led_l_walk.csv')\n",
    "\n",
    "abcd_d1a = pd.read_csv('./data/led_l_densbld.csv')\n",
    "\n",
    "abcd_fips = pd.read_csv('./data/merged_abcd_svi_with_fips.csv')\n",
    "\n",
    "print(\"Column names and count for abcd_walk:\")\n",
    "print(abcd_walk.columns)\n",
    "print(\"Total columns:\", len(abcd_walk))\n",
    "\n",
    "print(\"\\nColumn names and count for abcd_d1a:\")\n",
    "print(abcd_d1a.columns)\n",
    "print(\"Total columns:\", len(abcd_d1a))\n",
    "\n",
    "print(\"\\nColumn names and count for abcd_fips:\")\n",
    "print(abcd_fips.columns)\n",
    "print(\"Total columns:\", len(abcd_fips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "950e0761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the merged DataFrame:\n",
      "Index(['src_subject_id', 'STATE', 'COUNTY', 'FIPS', 'LOCATION', 'AREA_SQMI',\n",
      "       'reshist_addr1_svi_pov_20142018', 'reshist_addr1_svi_emp_20142018',\n",
      "       'reshist_addr1_svi_cap_20142018', 'reshist_addr1_svi_hs_20142018',\n",
      "       'reshist_addr1_svi_65_20142018', 'reshist_addr1_svi_17_20142018',\n",
      "       'reshist_addr1_svi_dis_20142018', 'reshist_addr1_svi_sin_20142018',\n",
      "       'reshist_addr1_svi_min_20142018', 'reshist_addr1_svi_eng_20142018',\n",
      "       'reshist_addr1_svi_hous20142018', 'reshist_addr1_svi_mob_20142018',\n",
      "       'reshist_addr1_svi_crwd20142018', 'reshist_addr1_svi_veh_20142018',\n",
      "       'reshist_addr1_svi_grp_20142018', 'EPL_POV', 'EPL_UNEMP', 'EPL_PCI',\n",
      "       'EPL_NOHSDP', 'EPL_AGE65', 'EPL_AGE17', 'EPL_DISABL', 'EPL_SNGPNT',\n",
      "       'EPL_MINRTY', 'EPL_LIMENG', 'EPL_MUNIT', 'EPL_MOBILE', 'EPL_CROWD',\n",
      "       'EPL_NOVEH', 'EPL_GROUPQ', 'eventname_x', 'reshist_addr1_walkindex',\n",
      "       'reshist_addr2_walkindex', 'reshist_addr3_walkindex', 'eventname_y',\n",
      "       'reshist_addr1_d1a', 'reshist_addr2_d1a', 'reshist_addr3_d1a'],\n",
      "      dtype='object')\n",
      "Total columns: 44\n"
     ]
    }
   ],
   "source": [
    "## left_join these three files based on src_subject_ID\n",
    "# Performing the left joins\n",
    "merged_df = abcd_fips.merge(abcd_walk, on='src_subject_id', how='left')\n",
    "merged_df = merged_df.merge(abcd_d1a, on='src_subject_id', how='left')\n",
    "\n",
    "# If you want to check the resulting DataFrame\n",
    "print(\"Column names in the merged DataFrame:\")\n",
    "print(merged_df.columns)\n",
    "print(\"Total columns:\", len(merged_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "119ae4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total columns: 11232\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_subject_id</th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>FIPS</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>AREA_SQMI</th>\n",
       "      <th>reshist_addr1_svi_pov_20142018</th>\n",
       "      <th>reshist_addr1_svi_emp_20142018</th>\n",
       "      <th>reshist_addr1_svi_cap_20142018</th>\n",
       "      <th>reshist_addr1_svi_hs_20142018</th>\n",
       "      <th>...</th>\n",
       "      <th>EPL_NOVEH</th>\n",
       "      <th>EPL_GROUPQ</th>\n",
       "      <th>eventname_x</th>\n",
       "      <th>reshist_addr1_walkindex</th>\n",
       "      <th>reshist_addr2_walkindex</th>\n",
       "      <th>reshist_addr3_walkindex</th>\n",
       "      <th>eventname_y</th>\n",
       "      <th>reshist_addr1_d1a</th>\n",
       "      <th>reshist_addr2_d1a</th>\n",
       "      <th>reshist_addr3_d1a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NDAR_INVRUVAZC2F</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>Maricopa</td>\n",
       "      <td>4.013817e+09</td>\n",
       "      <td>Census Tract 8169, Maricopa County, Arizona</td>\n",
       "      <td>9.429805</td>\n",
       "      <td>0.2554</td>\n",
       "      <td>0.3544</td>\n",
       "      <td>0.3086</td>\n",
       "      <td>0.0597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1080</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>baseline_year_1_arm_1</td>\n",
       "      <td>7.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baseline_year_1_arm_1</td>\n",
       "      <td>0.070879</td>\n",
       "      <td>0.070879</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDAR_INV65V1Z314</td>\n",
       "      <td>ARKANSAS</td>\n",
       "      <td>Pope</td>\n",
       "      <td>5.115951e+09</td>\n",
       "      <td>Census Tract 9512, Pope County, Arkansas</td>\n",
       "      <td>84.775572</td>\n",
       "      <td>0.6130</td>\n",
       "      <td>0.2996</td>\n",
       "      <td>0.5735</td>\n",
       "      <td>0.5116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0476</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>baseline_year_1_arm_1</td>\n",
       "      <td>5.667</td>\n",
       "      <td>5.667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baseline_year_1_arm_1</td>\n",
       "      <td>0.059590</td>\n",
       "      <td>0.059590</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NDAR_INV1T61M03G</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>6.001428e+09</td>\n",
       "      <td>Census Tract 4276, Alameda County, California</td>\n",
       "      <td>0.225045</td>\n",
       "      <td>0.7083</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.5585</td>\n",
       "      <td>0.8326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7307</td>\n",
       "      <td>0.5476</td>\n",
       "      <td>baseline_year_1_arm_1</td>\n",
       "      <td>15.667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baseline_year_1_arm_1</td>\n",
       "      <td>17.218853</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NDAR_INV8AWV40W3</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>6.001441e+09</td>\n",
       "      <td>Census Tract 4413.02, Alameda County, California</td>\n",
       "      <td>0.770145</td>\n",
       "      <td>0.4783</td>\n",
       "      <td>0.4346</td>\n",
       "      <td>0.1128</td>\n",
       "      <td>0.2822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0476</td>\n",
       "      <td>0.6536</td>\n",
       "      <td>baseline_year_1_arm_1</td>\n",
       "      <td>14.833</td>\n",
       "      <td>14.833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baseline_year_1_arm_1</td>\n",
       "      <td>6.103608</td>\n",
       "      <td>6.103608</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NDAR_INV75VFMGNT</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>6.001442e+09</td>\n",
       "      <td>Census Tract 4415.01, Alameda County, California</td>\n",
       "      <td>1.013511</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.3138</td>\n",
       "      <td>0.1571</td>\n",
       "      <td>0.1545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0334</td>\n",
       "      <td>0.4400</td>\n",
       "      <td>baseline_year_1_arm_1</td>\n",
       "      <td>8.500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baseline_year_1_arm_1</td>\n",
       "      <td>5.063131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     src_subject_id       STATE    COUNTY          FIPS  \\\n",
       "0  NDAR_INVRUVAZC2F     ARIZONA  Maricopa  4.013817e+09   \n",
       "1  NDAR_INV65V1Z314    ARKANSAS      Pope  5.115951e+09   \n",
       "2  NDAR_INV1T61M03G  CALIFORNIA   Alameda  6.001428e+09   \n",
       "3  NDAR_INV8AWV40W3  CALIFORNIA   Alameda  6.001441e+09   \n",
       "4  NDAR_INV75VFMGNT  CALIFORNIA   Alameda  6.001442e+09   \n",
       "\n",
       "                                           LOCATION  AREA_SQMI  \\\n",
       "0       Census Tract 8169, Maricopa County, Arizona   9.429805   \n",
       "1          Census Tract 9512, Pope County, Arkansas  84.775572   \n",
       "2     Census Tract 4276, Alameda County, California   0.225045   \n",
       "3  Census Tract 4413.02, Alameda County, California   0.770145   \n",
       "4  Census Tract 4415.01, Alameda County, California   1.013511   \n",
       "\n",
       "   reshist_addr1_svi_pov_20142018  reshist_addr1_svi_emp_20142018  \\\n",
       "0                          0.2554                          0.3544   \n",
       "1                          0.6130                          0.2996   \n",
       "2                          0.7083                          0.1238   \n",
       "3                          0.4783                          0.4346   \n",
       "4                          0.1209                          0.3138   \n",
       "\n",
       "   reshist_addr1_svi_cap_20142018  reshist_addr1_svi_hs_20142018  ...  \\\n",
       "0                          0.3086                         0.0597  ...   \n",
       "1                          0.5735                         0.5116  ...   \n",
       "2                          0.5585                         0.8326  ...   \n",
       "3                          0.1128                         0.2822  ...   \n",
       "4                          0.1571                         0.1545  ...   \n",
       "\n",
       "   EPL_NOVEH  EPL_GROUPQ            eventname_x  reshist_addr1_walkindex  \\\n",
       "0     0.1080      0.0000  baseline_year_1_arm_1                    7.000   \n",
       "1     0.0476      0.0000  baseline_year_1_arm_1                    5.667   \n",
       "2     0.7307      0.5476  baseline_year_1_arm_1                   15.667   \n",
       "3     0.0476      0.6536  baseline_year_1_arm_1                   14.833   \n",
       "4     0.0334      0.4400  baseline_year_1_arm_1                    8.500   \n",
       "\n",
       "   reshist_addr2_walkindex  reshist_addr3_walkindex            eventname_y  \\\n",
       "0                    7.000                      NaN  baseline_year_1_arm_1   \n",
       "1                    5.667                      NaN  baseline_year_1_arm_1   \n",
       "2                      NaN                      NaN  baseline_year_1_arm_1   \n",
       "3                   14.833                      NaN  baseline_year_1_arm_1   \n",
       "4                      NaN                      NaN  baseline_year_1_arm_1   \n",
       "\n",
       "   reshist_addr1_d1a  reshist_addr2_d1a  reshist_addr3_d1a  \n",
       "0           0.070879           0.070879                NaN  \n",
       "1           0.059590           0.059590                NaN  \n",
       "2          17.218853                NaN                NaN  \n",
       "3           6.103608           6.103608                NaN  \n",
       "4           5.063131                NaN                NaN  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Total columns:\", len(merged_df))\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b3d8575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows based on 'CensusTractFIPS', 'NatWalkInd', and 'D1A':\n",
      "Empty DataFrame\n",
      "Columns: [GEOID10, D1A, BlkGrpID, NatWalkInd]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "## before merging, check the duplicates within combined_data\n",
    "\n",
    "# Check for duplicates based on 'CensusTractFIPS', 'NatWalkInd', and 'D1A'\n",
    "duplicates = combined_data.duplicated(subset=['GEOID10', 'NatWalkInd', 'D1A'], keep=False)\n",
    "\n",
    "# Display all duplicate rows to understand the issue\n",
    "duplicate_rows = combined_data[duplicates]\n",
    "print(\"Duplicate rows based on 'CensusTractFIPS', 'NatWalkInd', and 'D1A':\")\n",
    "print(duplicate_rows) # 312 duplicates\n",
    "\n",
    "# write combined_data in csv\n",
    "#combined_data.to_csv('data/combined_walk_d1a_data.csv', index=False)\n",
    "\n",
    "# write duplicate_rows in csv\n",
    "duplicate_rows.to_csv('./data/duplicate_walk_d1a_data.csv',index=False)\n",
    "\n",
    "# write merged_df in csv\n",
    "merged_df.to_csv('data/merged_walk_d1a_abcd.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e45774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the source data and abcd data\n",
    "merged_abcd = pd.read_csv('data/merged_walk_d1a_abcd.csv')\n",
    "combined_data = pd.read_csv('data/combined_walk_d1a_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78c05f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['src_subject_id', 'STATE', 'COUNTY', 'FIPS', 'LOCATION', 'AREA_SQMI',\n",
      "       'reshist_addr1_svi_pov_20142018', 'reshist_addr1_svi_emp_20142018',\n",
      "       'reshist_addr1_svi_cap_20142018', 'reshist_addr1_svi_hs_20142018',\n",
      "       'reshist_addr1_svi_65_20142018', 'reshist_addr1_svi_17_20142018',\n",
      "       'reshist_addr1_svi_dis_20142018', 'reshist_addr1_svi_sin_20142018',\n",
      "       'reshist_addr1_svi_min_20142018', 'reshist_addr1_svi_eng_20142018',\n",
      "       'reshist_addr1_svi_hous20142018', 'reshist_addr1_svi_mob_20142018',\n",
      "       'reshist_addr1_svi_crwd20142018', 'reshist_addr1_svi_veh_20142018',\n",
      "       'reshist_addr1_svi_grp_20142018', 'EPL_POV', 'EPL_UNEMP', 'EPL_PCI',\n",
      "       'EPL_NOHSDP', 'EPL_AGE65', 'EPL_AGE17', 'EPL_DISABL', 'EPL_SNGPNT',\n",
      "       'EPL_MINRTY', 'EPL_LIMENG', 'EPL_MUNIT', 'EPL_MOBILE', 'EPL_CROWD',\n",
      "       'EPL_NOVEH', 'EPL_GROUPQ', 'eventname_x', 'reshist_addr1_walkindex',\n",
      "       'reshist_addr2_walkindex', 'reshist_addr3_walkindex', 'eventname_y',\n",
      "       'reshist_addr1_d1a', 'reshist_addr2_d1a', 'reshist_addr3_d1a', '_key',\n",
      "       'CensusTractFIPS'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Confirm column presence and create if necessary\n",
    "if 'FIPS' in merged_abcd.columns:\n",
    "    merged_abcd['CensusTractFIPS'] = merged_abcd['FIPS'].astype(str).str[:11]\n",
    "\n",
    "# Check that the column exists now\n",
    "print(merged_abcd.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32c788b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_abcd['CensusTractFIPS'] = merged_abcd['CensusTractFIPS'].astype(str)\n",
    "combined_data['CensusTractFIPS'] = combined_data['CensusTractFIPS'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e3e5c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching rows: 7978\n",
      "Number of NAs in key fields after merging: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Rename columns in combined_data for consistency in merging\n",
    "combined_data.rename(columns={'NatWalkInd': 'reshist_addr1_walkindex', 'D1A': 'reshist_addr1_d1a'}, inplace=True)\n",
    "\n",
    "# Drop rows with NaN in the key columns\n",
    "merged_abcd.dropna(subset=['reshist_addr1_walkindex', 'reshist_addr1_d1a'], inplace=True)\n",
    "combined_data.dropna(subset=['reshist_addr1_walkindex', 'reshist_addr1_d1a'], inplace=True)\n",
    "\n",
    "# Round the values to set a tolerance of 0.001\n",
    "tolerance = 0.001\n",
    "merged_abcd['reshist_addr1_walkindex'] = merged_abcd['reshist_addr1_walkindex'].round(4)\n",
    "merged_abcd['reshist_addr1_d1a'] = merged_abcd['reshist_addr1_d1a'].round(4)\n",
    "combined_data['reshist_addr1_walkindex'] = combined_data['reshist_addr1_walkindex'].round(4)\n",
    "combined_data['reshist_addr1_d1a'] = combined_data['reshist_addr1_d1a'].round(4)\n",
    "\n",
    "# Create a function to perform the custom merge\n",
    "def merge_with_tolerance(df1, df2, tol, on):\n",
    "    df1['_key'] = 1\n",
    "    df2['_key'] = 1\n",
    "    merged = pd.merge(df1, df2, on=['_key', 'CensusTractFIPS'])\n",
    "    merged = merged[(abs(merged['reshist_addr1_walkindex_x'] - merged['reshist_addr1_walkindex_y']) <= tol) &\n",
    "                    (abs(merged['reshist_addr1_d1a_x'] - merged['reshist_addr1_d1a_y']) <= tol)]\n",
    "    merged = merged.drop(columns=['_key', 'reshist_addr1_walkindex_y', 'reshist_addr1_d1a_y'])\n",
    "    merged = merged.rename(columns={'reshist_addr1_walkindex_x': 'reshist_addr1_walkindex', 'reshist_addr1_d1a_x': 'reshist_addr1_d1a'})\n",
    "    return merged\n",
    "\n",
    "# Perform the merge with tolerance\n",
    "result = merge_with_tolerance(merged_abcd, combined_data, tolerance, on=['CensusTractFIPS', 'reshist_addr1_walkindex', 'reshist_addr1_d1a'])\n",
    "\n",
    "# Count the number of matches\n",
    "num_matches = result.shape[0]\n",
    "\n",
    "print(f\"Number of matching rows: {num_matches}\")\n",
    "\n",
    "# Check the number of NAs in key fields after merging\n",
    "num_nas = result['GEOID10'].isna().sum()\n",
    "print(\"Number of NAs in key fields after merging:\", num_nas)\n",
    "\n",
    "# Filter rows with NAs in GEOID10\n",
    "nas_in_geo_id = result[result['GEOID10'].isna()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c475565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d2585d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in merged_abcd:\n",
      "Index(['src_subject_id', 'STATE', 'COUNTY', 'FIPS', 'LOCATION', 'AREA_SQMI',\n",
      "       'reshist_addr1_svi_pov_20142018', 'reshist_addr1_svi_emp_20142018',\n",
      "       'reshist_addr1_svi_cap_20142018', 'reshist_addr1_svi_hs_20142018',\n",
      "       'reshist_addr1_svi_65_20142018', 'reshist_addr1_svi_17_20142018',\n",
      "       'reshist_addr1_svi_dis_20142018', 'reshist_addr1_svi_sin_20142018',\n",
      "       'reshist_addr1_svi_min_20142018', 'reshist_addr1_svi_eng_20142018',\n",
      "       'reshist_addr1_svi_hous20142018', 'reshist_addr1_svi_mob_20142018',\n",
      "       'reshist_addr1_svi_crwd20142018', 'reshist_addr1_svi_veh_20142018',\n",
      "       'reshist_addr1_svi_grp_20142018', 'EPL_POV', 'EPL_UNEMP', 'EPL_PCI',\n",
      "       'EPL_NOHSDP', 'EPL_AGE65', 'EPL_AGE17', 'EPL_DISABL', 'EPL_SNGPNT',\n",
      "       'EPL_MINRTY', 'EPL_LIMENG', 'EPL_MUNIT', 'EPL_MOBILE', 'EPL_CROWD',\n",
      "       'EPL_NOVEH', 'EPL_GROUPQ', 'eventname_x', 'reshist_addr1_walkindex',\n",
      "       'reshist_addr2_walkindex', 'reshist_addr3_walkindex', 'eventname_y',\n",
      "       'reshist_addr1_d1a', 'reshist_addr2_d1a', 'reshist_addr3_d1a', '_key',\n",
      "       'CensusTractFIPS'],\n",
      "      dtype='object')\n",
      "Columns in combined_data:\n",
      "Index(['GEOID10', 'reshist_addr1_d1a', 'BlkGrpID', 'reshist_addr1_walkindex',\n",
      "       'CensusTractFIPS', '_key'],\n",
      "      dtype='object')\n",
      "Index(['src_subject_id', 'STATE', 'COUNTY', 'FIPS', 'LOCATION', 'AREA_SQMI',\n",
      "       'reshist_addr1_svi_pov_20142018', 'reshist_addr1_svi_emp_20142018',\n",
      "       'reshist_addr1_svi_cap_20142018', 'reshist_addr1_svi_hs_20142018',\n",
      "       'reshist_addr1_svi_65_20142018', 'reshist_addr1_svi_17_20142018',\n",
      "       'reshist_addr1_svi_dis_20142018', 'reshist_addr1_svi_sin_20142018',\n",
      "       'reshist_addr1_svi_min_20142018', 'reshist_addr1_svi_eng_20142018',\n",
      "       'reshist_addr1_svi_hous20142018', 'reshist_addr1_svi_mob_20142018',\n",
      "       'reshist_addr1_svi_crwd20142018', 'reshist_addr1_svi_veh_20142018',\n",
      "       'reshist_addr1_svi_grp_20142018', 'EPL_POV', 'EPL_UNEMP', 'EPL_PCI',\n",
      "       'EPL_NOHSDP', 'EPL_AGE65', 'EPL_AGE17', 'EPL_DISABL', 'EPL_SNGPNT',\n",
      "       'EPL_MINRTY', 'EPL_LIMENG', 'EPL_MUNIT', 'EPL_MOBILE', 'EPL_CROWD',\n",
      "       'EPL_NOVEH', 'EPL_GROUPQ', 'eventname_x', 'reshist_addr1_walkindex',\n",
      "       'reshist_addr2_walkindex', 'reshist_addr3_walkindex', 'eventname_y',\n",
      "       'reshist_addr1_d1a', 'reshist_addr2_d1a', 'reshist_addr3_d1a',\n",
      "       'CensusTractFIPS', 'GEOID10', 'BlkGrpID'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Print columns to confirm 'CensusTractFIPS' exists in both dataframes\n",
    "print(\"Columns in merged_abcd:\")\n",
    "print(merged_abcd.columns)\n",
    "\n",
    "print(\"Columns in combined_data:\")\n",
    "print(combined_data.columns)\n",
    "\n",
    "print(result.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f454898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check any of the CensusTractFIPS were the same as the ones in the duplicates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b11e0953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows not matched have been saved to unmatched_abcd.csv. Total unmatched rows: 3240\n"
     ]
    }
   ],
   "source": [
    "# Identify rows in merged_abcd that were not matched in the result\n",
    "merged_abcd['_merge_key'] = merged_abcd['CensusTractFIPS'].astype(str) + '-' + merged_abcd['reshist_addr1_walkindex'].astype(str) + '-' + merged_abcd['reshist_addr1_d1a'].astype(str)\n",
    "result['_merge_key'] = result['CensusTractFIPS'].astype(str) + '-' + result['reshist_addr1_walkindex'].astype(str) + '-' + result['reshist_addr1_d1a'].astype(str)\n",
    "\n",
    "# Find rows in merged_abcd not present in the result\n",
    "unmatched = merged_abcd[~merged_abcd['_merge_key'].isin(result['_merge_key'])]\n",
    "\n",
    "# Drop the _merge_key column before saving\n",
    "unmatched = unmatched.drop(columns=['_merge_key'])\n",
    "\n",
    "# Save the unmatched rows to a CSV file\n",
    "unmatched.to_csv('unmatched_abcd.csv', index=False)\n",
    "print(f\"Rows not matched have been saved to unmatched_abcd.csv. Total unmatched rows: {len(unmatched)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65046627",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combined_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## save the combined_data_duplicates to csv\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Find duplicate rows based on the specified columns\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m duplicates \u001b[38;5;241m=\u001b[39m combined_data[combined_data\u001b[38;5;241m.\u001b[39mduplicated(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc_subject_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreshist_addr1_walkindex\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreshist_addr1_d1a\u001b[39m\u001b[38;5;124m'\u001b[39m], keep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)]\n\u001b[0;32m      5\u001b[0m duplicates\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/second_duplicates.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mlen\u001b[39m(duplicates)\u001b[38;5;66;03m#34\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'combined_data' is not defined"
     ]
    }
   ],
   "source": [
    "## save the combined_data_duplicates to csv\n",
    "# Find duplicate rows based on the specified columns\n",
    "duplicates = combined_data[combined_data.duplicated(subset=['src_subject_id', 'reshist_addr1_walkindex', 'reshist_addr1_d1a'], keep=False)]\n",
    "\n",
    "duplicates.to_csv('./data/second_duplicates.csv',index=False)\n",
    "\n",
    "len(duplicates)#34\n",
    "\n",
    "# Check if any FIPS from result occur in duplicates\n",
    "result_fips_in_duplicates = result['CensusTractFIPS'].isin(duplicates['CensusTractFIPS'])\n",
    "\n",
    "# Filter to show only the rows from result that have FIPS occurring in duplicates\n",
    "fips_in_duplicates = result[result_fips_in_duplicates]\n",
    "\n",
    "# Display or save the filtered data\n",
    "print(fips_in_duplicates)\n",
    "\n",
    "## NONE of the Result were in Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d3100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for those unmatched\n",
    "## try another merge only using the first two digits of the FIPS and 'reshist_addr1_walkindex', 'reshist_addr1_d1a'\n",
    "\n",
    "len(unmatched) ## 481\n",
    "\n",
    "unmatched['CensusTractFIPS'] = unmatched['CensusTractFIPS'].astype(str)\n",
    "combined_data['CensusTractFIPS'] = combined_data['CensusTractFIPS'].astype(str)\n",
    "\n",
    "# Round the values to set a tolerance of 0.001\n",
    "unmatched['reshist_addr1_walkindex'] = unmatched['reshist_addr1_walkindex'].round(4)\n",
    "unmatched['reshist_addr1_d1a'] = unmatched['reshist_addr1_d1a'].round(4)\n",
    "combined_data['reshist_addr1_walkindex'] = combined_data['reshist_addr1_walkindex'].round(4)\n",
    "combined_data['reshist_addr1_d1a'] = combined_data['reshist_addr1_d1a'].round(4)\n",
    "\n",
    "# Extract the first two digits of the FIPS\n",
    "unmatched['CensusTractFIPS_short'] = unmatched['CensusTractFIPS'].str[:2]\n",
    "combined_data['CensusTractFIPS_short'] = combined_data['CensusTractFIPS'].str[:2]\n",
    "\n",
    "# Create a function to perform the custom merge\n",
    "def merge_with_tolerance(df1, df2, tol):\n",
    "    df1['_key'] = 1\n",
    "    df2['_key'] = 1\n",
    "    merged = pd.merge(df1, df2, on=['_key', 'CensusTractFIPS_short'])\n",
    "    merged = merged[(abs(merged['reshist_addr1_walkindex_x'] - merged['reshist_addr1_walkindex_y']) <= tol) &\n",
    "                    (abs(merged['reshist_addr1_d1a_x'] - merged['reshist_addr1_d1a_y']) <= tol)]\n",
    "    merged = merged.drop(columns=['_key', 'reshist_addr1_walkindex_y', 'reshist_addr1_d1a_y'])\n",
    "    merged = merged.rename(columns={'reshist_addr1_walkindex_x': 'reshist_addr1_walkindex', 'reshist_addr1_d1a_x': 'reshist_addr1_d1a'})\n",
    "    return merged\n",
    "\n",
    "# perform the merge with tolerance using shortened FIPS\n",
    "result_short_fips = merge_with_tolerance(unmatched, combined_data, tolerance)\n",
    "\n",
    "# Check the number of matches\n",
    "num_matches_short_fips = result_short_fips.shape[0]\n",
    "print(f\"Number of matching rows with short FIPS: {num_matches_short_fips}\")\n",
    "\n",
    "# Identify rows in unmatched that were still not matched\n",
    "unmatched['_merge_key'] = unmatched['CensusTractFIPS_short'].astype(str) + '-' + unmatched['reshist_addr1_walkindex'].astype(str) + '-' + unmatched['reshist_addr1_d1a'].astype(str)\n",
    "result_short_fips['_merge_key'] = result_short_fips['CensusTractFIPS_short'].astype(str) + '-' + result_short_fips['reshist_addr1_walkindex'].astype(str) + '-' + result_short_fips['reshist_addr1_d1a'].astype(str)\n",
    "\n",
    "# Find rows in unmatched not present in result_short_fips\n",
    "still_unmatched = unmatched[~unmatched['_merge_key'].isin(result_short_fips['_merge_key'])]\n",
    "\n",
    "# Drop the _merge_key column before saving\n",
    "still_unmatched = still_unmatched.drop(columns=['_merge_key'])\n",
    "\n",
    "# Save the still unmatched rows to a CSV file\n",
    "still_unmatched.to_csv('still_unmatched_abcd.csv', index=False)\n",
    "print(f\"Rows still not matched have been saved to still_unmatched_abcd.csv. Total still unmatched rows: {len(still_unmatched)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadb0998",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find duplicated rows for third match (third step)\n",
    "third_duplicates = combined_data[combined_data.duplicated(subset=['CensusTractFIPS_short', 'reshist_addr1_walkindex', 'reshist_addr1_d1a'], keep=False)]\n",
    "\n",
    "third_duplicates.to_csv('./data/third_duplicates_short.csv',index=False)\n",
    "len(third_duplicates) # 3233\n",
    "\n",
    "result_short_fips_in_duplicates = result_short_fips['GEOID10'].isin(second_duplicates['GEOID10'])\n",
    "\n",
    "    # Filter to show only the rows from result_short_fips that have GEOID10 occurring in duplicates\n",
    "fips_in_duplicates_short = result_short_fips[result_short_fips_in_duplicates]\n",
    "\n",
    "    # Display or save the filtered data\n",
    "print(fips_in_duplicates_short) # none of the fips in second_match were in the duplicates. \n",
    "\n",
    "result_short_fips.to_csv('second_match_state_abcd.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f84990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP FOUR \n",
    "\n",
    "# for those still unmatched (NAs for svi-derived fips)\n",
    "# try merging with combined_data only using reshist_addr1_walkindex and reshist_addr1_d1a and site state\n",
    "\n",
    "site_id = pd.read_excel('./data/subject_site_id.xlsx')\n",
    "\n",
    "# merge site_id and result_walkindex_d1a using src_subject_id\n",
    "# Merge the DataFrames on the 'src_subject_id' column\n",
    "result_site = pd.merge(site_id, still_unmatched, on='src_subject_id', how='inner')\n",
    "\n",
    "# Round the values to set a tolerance of 0.001\n",
    "result_site['reshist_addr1_walkindex'] = result_site['reshist_addr1_walkindex'].round(4)\n",
    "result_site['reshist_addr1_d1a'] = result_site['reshist_addr1_d1a'].round(4)\n",
    "result_site['CensusTractFIPS_short'] = result_site['site_state']\n",
    "\n",
    "# Convert the types of CensusTractFIPS_short to ensure they match\n",
    "result_site['CensusTractFIPS_short'] = result_site['CensusTractFIPS_short'].astype(str)\n",
    "combined_data['CensusTractFIPS_short'] = combined_data['CensusTractFIPS_short'].astype(str)\n",
    "\n",
    "tolerance=0.001\n",
    "\n",
    "# Create a function to perform the custom merge based only on walkindex and d1a\n",
    "def merge_with_result_site(df1, df2, tol):\n",
    "    df1['_key'] = 1\n",
    "    df2['_key'] = 1\n",
    "    merged = pd.merge(df1, df2, on=['_key', 'CensusTractFIPS_short'])\n",
    "    merged = merged[(abs(merged['reshist_addr1_walkindex_x'] - merged['reshist_addr1_walkindex_y']) <= tol) &\n",
    "                    (abs(merged['reshist_addr1_d1a_x'] - merged['reshist_addr1_d1a_y']) <= tol)]\n",
    "    merged = merged.drop(columns=['_key', 'reshist_addr1_walkindex_y', 'reshist_addr1_d1a_y'])\n",
    "    merged = merged.rename(columns={'reshist_addr1_walkindex_x': 'reshist_addr1_walkindex', 'reshist_addr1_d1a_x': 'reshist_addr1_d1a'})\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5168666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the merge with tolerance using only walkindex and d1a\n",
    "merge_3 = merge_with_result_site(result_site, combined_data, tolerance)\n",
    "\n",
    "# Check the number of matches\n",
    "num_matches_merge_3 = merge_3.shape[0]\n",
    "print(f\"Number of matching rows with walkindex and d1a: {num_matches_merge_3}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c3133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the duplicates and save to merge_3_no_duplicates\n",
    "# Remove duplicates\n",
    "merge_3_in_no_duplicates = merge_3.drop_duplicates()\n",
    "merge_3_in_no_duplicates.to_csv('./data/merge_3_no_duplicates.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cda1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Any still unmatched after the third match?, \n",
    "match1 = result\n",
    "match2 = result_short_fips\n",
    "match3 = merge_3_in_no_duplicates\n",
    "\n",
    "# all matches\n",
    "all_matches = pd.concat([match1['src_subject_id'], match2['src_subject_id'], match3['src_subject_id']]).drop_duplicates()\n",
    "\n",
    "# Identify unmatched src_subject_id values\n",
    "unmatched_ids = merged_df[~merged_df['src_subject_id'].isin(all_matches)]\n",
    "\n",
    "# Display the number of matched and unmatched ids\n",
    "num_matched = all_matches.shape[0]\n",
    "num_unmatched = unmatched_ids.shape[0]\n",
    "\n",
    "print(f\"Number of matched src_subject_id values: {num_matched}\")\n",
    "print(f\"Number of unmatched src_subject_id values: {num_unmatched}\")\n",
    "\n",
    "# Optionally, save the unmatched src_subject_id values to a CSV file\n",
    "unmatched_ids.to_csv('unmatched_src_subject_id.csv', index=False) # 103 remains\n",
    "# successfully matched 11113 participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648b3af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge three matches together \n",
    "# Combine matched datasets by merging on 'src_subject_id'\n",
    "combined_horizontal = pd.concat([match1, match2, match3], axis=0)\n",
    "\n",
    "# Print the column names\n",
    "print(combined_horizontal.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c8e1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain only the specified columns\n",
    "columns_to_retain = [\n",
    "  'src_subject_id', 'STATE', 'COUNTY', 'CensusTractFIPS', 'LOCATION', 'AREA_SQMI',\n",
    "'BlkGrpID', '_merge_key', 'CensusTractFIPS_x', 'CensusTractFIPS_short', \n",
    "  'CensusTractFIPS_y', 'site', 'site_state', 'site_state_name'\n",
    "]\n",
    "\n",
    "## Check filtered_combined_matches (any duplicates of src_subject_id)\n",
    "duplicates = filtered_combined_matches[filtered_combined_matches.duplicated(subset='src_subject_id', keep=False)]\n",
    "\n",
    "# Print the number of duplicate rows and the duplicate rows themselves\n",
    "num_duplicates = duplicates.shape[0]\n",
    "print(f\"Number of duplicate src_subject_id values: {num_duplicates}\")\n",
    "print(duplicates)\n",
    "\n",
    "no_duplicates_combined_matches = filtered_combined_matches.drop_duplicates(subset='src_subject_id', keep=False)\n",
    "len(no_duplicates_combined_matches) # 11098 \n",
    "\n",
    "# save the final dataset\n",
    "no_duplicates_combined_matches.to_csv('abcd_blkgrp_matches_nodup.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cfebcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
